{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1782e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.draw import circle_perimeter\n",
    "from math import sqrt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import threshold_otsu\n",
    "from tabulate import tabulate\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "import h5py    # for reading HDF5 files \n",
    "import cv2     # for image conversion\n",
    "import re    # use it to sort but not sure what it is!\n",
    "from scipy.stats import zscore\n",
    "from skimage import draw, measure\n",
    "import pywt\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44901c-ca74-4b9f-bfab-5848a68a842a",
   "metadata": {},
   "source": [
    "# Data extraction #\n",
    " \n",
    "Data set infromation overview:\n",
    "\n",
    "1. Divergent Sets:\n",
    "\n",
    "    Consists of 12 genotypically distinct strains. These are strains that are further apart on the phenotypic tree. \n",
    "    Ideal for determining broad-sense heritability with repeated measurements of specific phenotypes.\n",
    "    This allows us to look into the relationship between the behavior and the genetic makeup. \n",
    "\n",
    "2. Mapping Sets:\n",
    "\n",
    "    Comprises 48 strains suitable for collecting phenotype data for broad diversity or genome-wide association studies.\n",
    "    Using multiple strain sets can enhance the statistical strength in genome-wide association studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa57fb-48d3-44a6-8294-99921452263a",
   "metadata": {},
   "source": [
    "### Pipline: ###\n",
    "\n",
    "1. Take hdf5 tierpsy output and extract frames\n",
    "\n",
    "ATTENTION: change the output_root_path depending on which data set you are using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaf66f6-ad0e-47df-a530-3ac70446a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_path(file_path):\n",
    "    # Get the base file name\n",
    "    base_name = os.path.basename(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Find the position of the first underscore after the 6th character\n",
    "        first_underscore_pos = base_name.index('_', 6)\n",
    "        \n",
    "        # Extract the label from the 6th character to the first underscore\n",
    "        label_strain = base_name[6:first_underscore_pos]\n",
    "        \n",
    "        return label_strain\n",
    "    except ValueError:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb76a7d-e9eb-4289-9985-0a08a52a44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where figures will be saved\n",
    "output_root_path = r'/Volumes/TOSHIBA_EXT/Phenotype_features_collective/Data/frames_extracted/divergent_set'\n",
    "\n",
    "# Create the output root folder if it doesn't exist\n",
    "if not os.path.exists(output_root_path):\n",
    "    os.makedirs(output_root_path)\n",
    "\n",
    "# List of file paths for HDF5 files you want to load\n",
    "hdf5_file_paths = [\n",
    "    # npr1, N2, CB4856    \n",
    "    \n",
    "    # divergent set \n",
    "   \"/Volumes/TOSHIBA_EXT/Phenotype_features_collective/Data/1.1_1_da609_Set0_Pos0_Ch5_14012018_092702.hdf5\",\n",
    "   \"/Volumes/TOSHIBA_EXT/Phenotype_features_collective/Data/1.1_1_eca246_8f_Set0_Pos0_Ch3_14012018_093713.hdf5\",\n",
    "   '/Volumes/TOSHIBA_EXT/Phenotype_features_collective/Data/1.1_1_nic252_c2_Set0_Pos0_Ch1_14012018_094825.hdf5'\n",
    "]\n",
    "\n",
    "labels_strain_list = {path: extract_label_from_path(path) for path in hdf5_file_paths}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767c4ec-bba4-4861-b28d-28557ec5c0e8",
   "metadata": {},
   "source": [
    "Extract frames from each hdf5 file: \n",
    "\n",
    "\n",
    "This code snippet is designed to process a series of HDF5 files, specifically for extracting and saving image data:\n",
    "\n",
    "1. Iterate Over HDF5 Files: The for loop iterates over a list of file paths (hdf5_file_paths).\n",
    "\n",
    "2. Open HDF5 File: Inside the loop, each file is opened using h5py.File.\n",
    "\n",
    "3. Access Image Dataset: The script accesses a dataset named /mask within each HDF5 file. This dataset presumably contains image data. Information about the dataset, like its shape and data type, is printed out.\n",
    "\n",
    "4. Frame Extraction Step: A variable step is set, which determines the interval at which frames (or elements) from the dataset will be extracted. In this code, it's set to 500, meaning every 500th frame will be considered.\n",
    "\n",
    "5. Output Directory Preparation: For each HDF5 file, an output directory is created to store the extracted images. This directory is based on the name of the HDF5 file and is located within a root output path (output_root_path).\n",
    "\n",
    "6. Create Directory If Necessary: The script checks if the output directory already exists. If it doesn't, it creates the directory using os.makedirs.\n",
    "\n",
    "7. Extract and Save Frames: Another loop iterates through the image dataset (img_ds2), jumping step frames at a time. Each selected frame is saved as a JPEG image. The naming convention for these images is based on their index in the dataset, formatted to have six digits with leading zeros if necessary.\n",
    "\n",
    "This code is useful for batch processing HDF5 files containing image data, particularly when you only need to extract and save certain frames (e.g., for donsampling or reducing data size).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fc27d7-8723-4d16-a518-b94ac24e489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Dataset info: Shape=(67509, 2048, 2048), Dtype=uint8\n",
      "Image Dataset info: Shape=(67506, 2048, 2048), Dtype=uint8\n",
      "Image Dataset info: Shape=(67510, 2048, 2048), Dtype=uint8\n"
     ]
    }
   ],
   "source": [
    "# Loop through the list of HDF5 file paths\n",
    "for file_path in hdf5_file_paths:\n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "        \n",
    "        # Get image dataset\n",
    "        img_ds2 = hdf['/mask']\n",
    "        print(f'Image Dataset info: Shape={img_ds2.shape}, Dtype={img_ds2.dtype}')\n",
    "\n",
    "        # Set the step value to control how many frames are extracted\n",
    "        step = 500  # Change this value to extract frames at a different interval\n",
    "\n",
    "        # Create a unique output directory based on the file name\n",
    "        file_name = os.path.basename(file_path)\n",
    "        output_directory = os.path.join(output_root_path, os.path.splitext(file_name)[0])\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "\n",
    "        # Loop to extract frames from the current HDF5 file based on the step value\n",
    "        for i in range(0, img_ds2.shape[0], step):  # Loop through frames with a step\n",
    "            name = \"{:06d}\".format(i)\n",
    "            cv2.imwrite(os.path.join(output_directory, f'{name}.jpg'), img_ds2[i, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6b91f-cbfa-490c-b7bf-ebb9fc381d3e",
   "metadata": {},
   "source": [
    "### Load all image files in the different folders ###\n",
    "\n",
    "This script is designed to process and organize images extracted from HDF5 files:\n",
    "\n",
    "1. Root Directory Specification: The script starts by defining a root_directory, which is the same as output_root_path. This directory contains subdirectories, each corresponding to a specific HDF5 file.\n",
    "\n",
    "2. List Subdirectories: It generates a list of subdirectories in the root directory. Each subdirectory represents a different HDF5 file and contains image files extracted from that HDF5 file.\n",
    "\n",
    "3. Initialize Image Collection: An empty list all_images is initialized to store images from each HDF5 file.\n",
    "\n",
    "4. Process Each Subdirectory: The script iterates over each subdirectory. For each subdirectory:\n",
    "   \n",
    "    a. It resets the imagefiles list.\n",
    "    \n",
    "    b. Changes the working directory to the current subdirectory.\n",
    "    \n",
    "    c. Collects all JPEG files in this directory, sorting them to ensure they are in the correct order.\n",
    "    \n",
    "    d. Image Loading and Selection: variable every_jth_image is set to 10, which determines the interval at which images will be selected. The script ensures to include the first and the last frame from each subdirectory. It then loads every 10th image (based on every_jth_image value) from the sorted list of JPEG files. These images are loaded in grayscale mode using cv2.imread. Store Processed Images: The images from each subdirectory (first frame, every 10th frame, and last frame) are appended to the all_images list.\n",
    "\n",
    "5. Output Summary: Finally, the script prints the total number of HDF5 files processed, inferred from the length of the all_images list.\n",
    "\n",
    "This script is particularly useful for downsampling image data from a larger set, ensuring that key frames (the first and last) are included. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Root directory where subdirectories for each HDF5 file are located\n",
    "# root_directory = output_root_path\n",
    "\n",
    "# # Get a list of subdirectories (each subdirectory corresponds to an HDF5 file)\n",
    "# subdirectories = [os.path.join(root_directory, d) for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))]\n",
    "\n",
    "# # A list to store images from each HDF5 file\n",
    "# all_images = []\n",
    "\n",
    "# for subdirectory in subdirectories:\n",
    "#     # Reset the list of image files for each subdirectory\n",
    "#     imagefiles = []\n",
    "\n",
    "#     # Change directory to the subdirectory\n",
    "#     os.chdir(subdirectory)\n",
    "\n",
    "#     # Get image files in the current subdirectory\n",
    "#     imagefiles = [file for file in os.listdir() if file.endswith(\".jpg\")]\n",
    "#     imagefiles.sort()  # Sort the files to ensure they are in the correct order\n",
    "\n",
    "#     images = []  # List to store images from the current subdirectory\n",
    "#     every_jth_image = 10  # Set your interval here\n",
    "\n",
    "#     # Always include the first frame\n",
    "#     first_frame = cv2.imread(imagefiles[0], cv2.IMREAD_GRAYSCALE)\n",
    "#     if first_frame is not None:\n",
    "#         images.append(first_frame)\n",
    "\n",
    "#     # Load images at intervals defined by every_jth_image\n",
    "#     for ii in range(every_jth_image, len(imagefiles) - 1, every_jth_image):\n",
    "#         currentfilename = imagefiles[ii]\n",
    "#         currentimage = cv2.imread(currentfilename, cv2.IMREAD_GRAYSCALE)\n",
    "#         if currentimage is not None:\n",
    "#             images.append(currentimage)\n",
    "\n",
    "#     # Always include the last frame\n",
    "#     last_frame = cv2.imread(imagefiles[-1], cv2.IMREAD_GRAYSCALE)\n",
    "#     if last_frame is not None:\n",
    "#         images.append(last_frame)\n",
    "\n",
    "#     all_images.append(images)  # Append the list of images from the current subdirectory to the all_images list\n",
    "\n",
    "# print(\"Total number of HDF5 files processed: \", len(all_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean variable to decide to load from folder or from HDF5\n",
    "load_from_folder = False\n",
    "\n",
    "def load_images_from_folder(main_folder, every_jth_image=10):\n",
    "    \"\"\"\n",
    "    Load images from each subfolder within the main folder.\n",
    "    Each subfolder's images are stored in a separate list.\n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    subfolders = [os.path.join(main_folder, d) for d in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, d))]\n",
    "\n",
    "    for folder in subfolders:\n",
    "        imagefiles = [file for file in os.listdir(folder) if file.endswith(\".jpg\")]\n",
    "        imagefiles.sort()  # Sort the files to ensure they are in the correct order\n",
    "\n",
    "        images = []\n",
    "        for ii in range(0, len(imagefiles), every_jth_image):\n",
    "            currentfilename = imagefiles[ii]\n",
    "            currentimage = cv2.imread(os.path.join(folder, currentfilename), cv2.IMREAD_GRAYSCALE)\n",
    "            if currentimage is not None:\n",
    "                images.append(currentimage)\n",
    "\n",
    "        all_images.append(images)\n",
    "\n",
    "    return all_images\n",
    "\n",
    "# Main directory containing the subfolders with images\n",
    "main_folder = \"/Volumes/TOSHIBA EXT/Phenotype_features_collective/Data/frames_extracted/divergent_set\"\n",
    "\n",
    "# Load images from subfolders\n",
    "all_images = load_images_from_folder(main_folder)\n",
    "\n",
    "print(\"Total number of subfolders processed: \", len(all_images))\n",
    "\n",
    "\n",
    "def load_images_from_hdf5(subdirectory, every_jth_image=10):\n",
    "    \"\"\"\n",
    "    Load images from an HDF5 file, selecting every jth image.\n",
    "    \"\"\"\n",
    "    # Reset the list of image files for each subdirectory\n",
    "    imagefiles = [file for file in os.listdir(subdirectory) if file.endswith(\".jpg\")]\n",
    "    imagefiles.sort()  # Sort the files to ensure they are in the correct order\n",
    "\n",
    "    images = []\n",
    "    if len(imagefiles) > 0:\n",
    "        first_frame = cv2.imread(os.path.join(subdirectory, imagefiles[0]), cv2.IMREAD_GRAYSCALE)\n",
    "        if first_frame is not None:\n",
    "            images.append(first_frame)\n",
    "\n",
    "        for ii in range(every_jth_image, len(imagefiles) - 1, every_jth_image):\n",
    "            currentfilename = imagefiles[ii]\n",
    "            currentimage = cv2.imread(os.path.join(subdirectory, currentfilename), cv2.IMREAD_GRAYSCALE)\n",
    "            if currentimage is not None:\n",
    "                images.append(currentimage)\n",
    "\n",
    "        last_frame = cv2.imread(os.path.join(subdirectory, imagefiles[-1]), cv2.IMREAD_GRAYSCALE)\n",
    "        if last_frame is not None:\n",
    "            images.append(last_frame)\n",
    "\n",
    "    return images\n",
    "\n",
    "# Root directory where subdirectories for each HDF5 file are located\n",
    "root_directory = output_root_path\n",
    "\n",
    "# Get a list of subdirectories (each subdirectory corresponds to an HDF5 file or a folder of images)\n",
    "subdirectories = [os.path.join(root_directory, d) for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))]\n",
    "\n",
    "all_images = []\n",
    "\n",
    "for subdirectory in subdirectories:\n",
    "    if load_from_folder:\n",
    "        images = load_images_from_folder(subdirectory)\n",
    "    else:\n",
    "        images = load_images_from_hdf5(subdirectory)\n",
    "    \n",
    "    all_images.append(images)\n",
    "\n",
    "print(\"Total number of HDF5 files or folders processed: \", len(all_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563983c-972a-4ee5-9cbc-243ae774f6f6",
   "metadata": {},
   "source": [
    "We maintain the all_images list, which will contain lists of images for each HDF5 file.\n",
    "Inside the loop for each subdirectory, we initialize an empty images list to store images from the current subdirectory.\n",
    "\n",
    "After processing all images in a subdirectory, we append the images list to the all_images list. Each images list corresponds to images from one HDF5 file.\n",
    "\n",
    "The all_images list will have one inner list for each HDF5 file, and you can access images from a specific file by indexing all_images.\n",
    "\n",
    "At the end of this code, all_images will contain lists of images, and the length of all_images will indicate how many HDF5 files were processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6518a-76b1-4711-8144-b0cc8828a8ec",
   "metadata": {},
   "source": [
    "# Test: Segmentation, cm detection in 1 frame per video #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all frames for all videos ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dab3f-c368-4001-9d05-b3f48cb8aca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each list of images (each list corresponds to a video)\n",
    "for idx, video_images in enumerate(all_images[:]):\n",
    "    num_frames = len(video_images)\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplot\n",
    "    num_cols = 5  \n",
    "    num_rows = int(np.ceil(num_frames / num_cols))\n",
    "    \n",
    "    # Create a new figure for each video\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 15))\n",
    "    \n",
    "    # Get the file path corresponding to the current set of images\n",
    "    file_path = hdf5_file_paths[idx]\n",
    "    \n",
    "    # Get the label for this file from the labels dictionary\n",
    "    label_file = labels_strain_list[file_path]\n",
    "    \n",
    "    # Flatten the axs array if it's 2D\n",
    "    axs = axs.ravel() if num_rows > 1 else [axs]\n",
    "    \n",
    "    # Set the title for the figure using the label\n",
    "    axs[0].set_title(f\"Video from {label_file}\")  # Set title on the first subplot\n",
    "    \n",
    "    # Plot each frame\n",
    "    for i, ax in enumerate(axs):\n",
    "        if i < num_frames:\n",
    "            ax.imshow(video_images[i], cmap='gray')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')  # Turn off extra subplots\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for title\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13142ba7-187f-4323-b436-88bc53c5dc31",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2eab8",
   "metadata": {},
   "source": [
    "\n",
    "This script is designed to analyze specific frames from a collection of images, focusing on detecting and visualizing circular regions within each frame. It is a common method used in image processing. First we treshold the image, then we binarize the image so that we have black background and white blobs. This is then used for segmentation. \n",
    "\n",
    "1. Specify Frame to Analyze: The script starts by defining frame_chosen, an index specifying the particular frame to analyze across different image sets.\n",
    "\n",
    "2. Initialization: Lists all_centers and all_radii are initialized to store the centers and radii of detected circular regions for each frame. An index idx is also initialized.\n",
    "\n",
    "3. Validation: The script first checks if frame_chosen is within a valid range for the first sublist in all_images. This assumes all sublists have the same length or at least lengths greater than frame_chosen.\n",
    "\n",
    "4. Process Each Image Set: The script iterates over each sublist in all_images:\n",
    "\n",
    "    a. It selects the frame at index frame_chosen from the sublist.\n",
    "\n",
    "    b. The chosen frame undergoes thresholding using Otsu's method to convert it into a binary image.\n",
    "\n",
    "    c. It then labels connected components in the binary image.\n",
    "    \n",
    "    d. Region properties are extracted from the labeled image.\n",
    "5. Region Filtering and Data Extraction: The script filters regions based on a threshold area size and extracts the centroid and diameter of each region:\n",
    "\n",
    "    a. A threshold area (threshold_area) is defined.\n",
    "    b. Regions smaller than this threshold are discarded.\n",
    "    c. Centers and diameters of the remaining regions are calculated.\n",
    "\n",
    "6. Radii Calculation and Storage: Radii of the detected regions are calculated and stored in all_radii, while their centers are stored in all_centers.\n",
    "\n",
    "7. Visualization: The script plots three images side by side:\n",
    "\n",
    "    a. The binary image (after thresholding).\n",
    "    b. The labeled image (showing connected components).\n",
    "    c. The original image with detected circles overlaid.\n",
    "    d. The subplot titles are set, and for the third image, circles are drawn around detected regions.\n",
    "    e. The script also retrieves a label for the current set of images from hdf5_file_paths using a function extract_label_from_path, which is not provided in the snippet but is assumed to extract meaningful labels from file paths.\n",
    "8. Range Check: If frame_chosen is out of the valid range, a message is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347e84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the frame you want to analyze (0-based index)\n",
    "frame_chosen = 14  # Change this to the desired frame number\n",
    "\n",
    "# Lists to store centers and radii for each frame\n",
    "all_centers = []\n",
    "all_radii = []\n",
    "idx = 0\n",
    "\n",
    "# Ensure frame_chosen is within a valid range for all sublists\n",
    "if frame_chosen >= 0 and frame_chosen < len(all_images[0]):\n",
    "    # Loop through all sublists of images contained in all_images\n",
    "    for sublist in all_images:\n",
    "        if frame_chosen < len(sublist):\n",
    "            # Retrieve the chosen frame from the sublist\n",
    "            image = sublist[frame_chosen]\n",
    "\n",
    "            # Perform thresholding on the current image\n",
    "            threshold = threshold_otsu(image)\n",
    "            binary_image = image > threshold\n",
    "\n",
    "            # Label connected components in the binary image\n",
    "            label_image = label(binary_image)\n",
    "\n",
    "            # Color-code the labeled image\n",
    "            color_label_image = color.label2rgb(label_image, image=image, bg_label=0)\n",
    "\n",
    "            # Extract region properties from labeled image\n",
    "            props = regionprops(label_image)\n",
    "            centers = []\n",
    "            diameters = []\n",
    "\n",
    "            # Define the threshold area size\n",
    "            threshold_area = 10  # Set the minimum area size to keep\n",
    "\n",
    "            # Create a new list to store the filtered region properties\n",
    "            filtered_props = []\n",
    "\n",
    "            # Iterate over the region properties\n",
    "            for prop in props:\n",
    "                # Check the area size of the current region property\n",
    "                if prop.area >= threshold_area:\n",
    "                    # If the area size meets the threshold, add it to the filtered_props list\n",
    "                    filtered_props.append(prop)\n",
    "\n",
    "            # Extract centers and diameters of the detected regions\n",
    "            for prop in filtered_props:\n",
    "                centers.append(prop.centroid)\n",
    "                diameters.append(np.mean([prop.major_axis_length, prop.minor_axis_length]))\n",
    "\n",
    "            radii = np.array(diameters) / 2\n",
    "\n",
    "            # Append centers and radii to the respective lists for this frame\n",
    "            all_centers.append(centers)\n",
    "            all_radii.append(radii)\n",
    "\n",
    "            # Plot the images with detected circles and color-coded labeled image\n",
    "            plt.figure(figsize=(18, 6))\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(binary_image, cmap='gray')\n",
    "            plt.title('Binary Image')\n",
    "\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(color_label_image)\n",
    "            plt.title('Connected areas color-coded')\n",
    "\n",
    "            plt.subplot(133)\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            for center, radius in zip(centers, radii):\n",
    "                circle = plt.Circle((center[1], center[0]), radius, edgecolor='r', facecolor='none')\n",
    "                plt.gca().add_patch(circle)\n",
    "            # Get the file path corresponding to the current set of images\n",
    "            file_path = hdf5_file_paths[idx]\n",
    "            idx = idx + 1\n",
    "\n",
    "            # Get the label for this file using a function (assuming it's defined)\n",
    "            label_file = extract_label_from_path(file_path)\n",
    "\n",
    "            # Use the label in your plot title\n",
    "            plt.title(f'Image with Detected Circles - {label_file}')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"frame_chosen is out of range for the available frames in the sublists.\")\n",
    "\n",
    "# Now, all_centers and all_radii contain the data for each frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of frames and number of detected regions ###\n",
    "The provided code snippet is designed to analyze the number of frames and the number of detected regions per frame in a set of images.\n",
    "\n",
    "1. Determine Number of Frames: The code starts by finding the length of the all_radii list, which is stored in num_frames. This number represents the total number of frames across all sublists in all_images for which centers and radii of detected regions have been stored.\n",
    "\n",
    "2. Iterate Through Videos/Images: The script iterates over all_images, which is a list of sublists, with each sublist corresponding to a different video or image set. enumerate is used to get both the index (video_i) and the sublist itself.\n",
    "\n",
    "3. Check Frame Count: Within the loop, it first checks if num_frames is greater than zero, indicating that there are frames processed.\n",
    "\n",
    "4. Number of Detected Regions per Frame: For each sublist (video or image set), it calculates the number of detected regions in a specific frame, which is given by the length of the sublist in all_radii at the index video_i. This value is stored in num_regions_per_frame.\n",
    "\n",
    "5. Print Information: The script then prints the total number of frames (num_frames) and the number of detected regions per frame (num_regions_per_frame) for each video or image set.\n",
    "\n",
    "6. Empty List Check: If num_frames is zero, indicating that all_radii is empty (no frames were processed or no regions were detected), it prints \"all_centers is empty.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef38ad3-98a6-49af-8391-adb930a06208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of frames and detected regions per frame in all_centers\n",
    "num_frames = len(all_radii)\n",
    "for video_i, sublist in enumerate(all_images):\n",
    "    if num_frames > 0:\n",
    "        num_regions_per_frame = len(all_radii[video_i])\n",
    "        print(\"Number of Frames:\", num_frames)\n",
    "        print(\"Number of Detected Regions per Frame:\", num_regions_per_frame)\n",
    "    else:\n",
    "        print(\"all_centers is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob cm #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c279ef",
   "metadata": {},
   "source": [
    "This code snippet performs image processing and analysis on all frames from each sublist of images, focusing on region properties, particularly the centroids and their distances from the center of the image. \n",
    "1. The first part of the code extracts and stores detailed region properties (normalized centers and radii from the center) for a specific frame in each HDF5 file.\n",
    "    a. Initialize Lists: Two lists all_centers_norm and all_radius_from_center are initialized to store normalized center coordinates and radii from the center for each HDF5 file.\n",
    "    b. Iterate Through Sublists: \n",
    "        i. For each sublist in all_images, it checks if frame_i is a valid index.\n",
    "        ii. Performs thresholding on the selected frame and labels the connected components to create label_image.\n",
    "        iii. Calculates the properties of each labeled region using regionprops.\n",
    "        iv. Normalized Centers: The centroids of these regions are normalized by subtracting the center of the image (calculated as half of the image dimensions).\n",
    "        v. Radius from Center: The Euclidean distance of each normalized centroid from the center of the image is calculated.\n",
    "\n",
    "These normalized centers and radii are appended to all_centers_norm and all_radius_from_center, respectively.\n",
    "\n",
    "2. The second part calculates and stores the mean of these properties for each HDF5 file.\n",
    "    a. Initialize Lists for Means: Two new lists, mean_centers_norm_per_hdf5 and mean_radius_from_center_per_hdf5, are initialized to store the mean normalized centers and mean radii for each HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data for each HDF5 file\n",
    "all_centers_norm_per_hdf5 = []\n",
    "all_radius_from_center_per_hdf5 = []\n",
    "\n",
    "# Initialize lists to store mean and standard deviation data for each HDF5 file\n",
    "mean_centers_norm_per_hdf5 = []\n",
    "std_radius_from_center_per_hdf5 = []\n",
    "\n",
    "# Iterate through each sublist corresponding to an HDF5 file\n",
    "for sublist in all_images:\n",
    "    # Initialize lists to store data for each frame in the current HDF5 file\n",
    "    all_centers_norm = []\n",
    "    all_radius_from_center = []\n",
    "\n",
    "    # Iterate through each frame in the sublist\n",
    "    for frame in sublist:\n",
    "        # Label the image for the current frame\n",
    "        label_image = label(frame > threshold_otsu(frame))\n",
    "\n",
    "        # Calculate the region properties\n",
    "        stats = regionprops(label_image)\n",
    "\n",
    "        # Calculate the normalized centers\n",
    "        centers_norm = np.array([prop.centroid for prop in stats]) - np.round(np.array(frame.shape) / 2)\n",
    "\n",
    "        # Calculate the radius from the center\n",
    "        radius_from_center = np.sqrt(np.sum(centers_norm ** 2, axis=1))\n",
    "\n",
    "        # Append centers_norm and radius_from_center to the respective lists\n",
    "        all_centers_norm.append(centers_norm)\n",
    "        all_radius_from_center.append(radius_from_center)\n",
    "\n",
    "    # Store data for all frames of the current HDF5 file\n",
    "    all_centers_norm_per_hdf5.append(all_centers_norm)\n",
    "    all_radius_from_center_per_hdf5.append(all_radius_from_center)\n",
    "\n",
    "    # Calculate and store mean and standard deviation data for the current HDF5 file\n",
    "    mean_centers_norm = np.mean(np.vstack(all_centers_norm), axis=0)\n",
    "    std_radius_from_center = np.std(np.hstack(all_radius_from_center))\n",
    "\n",
    "    mean_centers_norm_per_hdf5.append(mean_centers_norm)\n",
    "    std_radius_from_center_per_hdf5.append(std_radius_from_center)\n",
    "\n",
    "# Now, all_centers_norm_per_hdf5 and all_radius_from_center_per_hdf5 contain lists of data for each frame in each HDF5 file.\n",
    "# mean_centers_norm_per_hdf5 contains the means of centers_norm per HDF5 file.\n",
    "# std_radius_from_center_per_hdf5 contains the standard deviations of radius_from_center per HDF5 file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6cd48-6d6a-4b5d-a037-df3cfce20d9b",
   "metadata": {},
   "source": [
    "### Violin plot of blob cm to center image distance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872c755-cf00-4f8f-8670-324bc61b76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of datasets\n",
    "n_datasets = len(all_radius_from_center_per_hdf5)\n",
    "\n",
    "# Create a new figure for the violin plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Flatten the data for violin plot\n",
    "# Each sublist in 'all_radius_from_center_per_hdf5' contains lists for each frame; we concatenate them into a single list per HDF5 file\n",
    "violin_data = [np.concatenate(radius_list) for radius_list in all_radius_from_center_per_hdf5]\n",
    "\n",
    "# Extract labels for each dataset from hdf5_file_paths or another source\n",
    "# This assumes a function 'extract_label_from_path' that extracts a meaningful label from the file path\n",
    "labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "# Create violin plot using seaborn\n",
    "sns.violinplot(data=violin_data, inner=\"quartile\", scale=\"count\")\n",
    "\n",
    "# Add individual data points as a swarmplot\n",
    "sns.swarmplot(data=violin_data, color=\"k\", alpha=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Blob Radius Distances from Image Center')\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('Distance from Image Center')\n",
    "plt.xticks(ticks=range(len(labels)), labels=labels)  # Set x-axis labels to actual dataset names\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series plot ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels for each dataset\n",
    "labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "# Create a single figure for all time series\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plotting radius from center with standard deviation as a shadowed area for each dataset\n",
    "for idx, (dataset, std_dev) in enumerate(zip(all_radius_from_center_per_hdf5, std_radius_from_center_per_hdf5)):\n",
    "    radius_data = np.array([np.mean(frame) if len(frame) > 0 else np.nan for frame in dataset])\n",
    "    std_dev_data = np.array([np.std(frame) if len(frame) > 0 else np.nan for frame in dataset])\n",
    "\n",
    "    frame_numbers = np.arange(len(radius_data))\n",
    "\n",
    "    # Plotting the mean data\n",
    "    dataset_label = labels[idx] if idx < len(labels) else f\"Dataset {idx + 1}\"\n",
    "    plt.plot(frame_numbers, radius_data, label=dataset_label)\n",
    "\n",
    "    # Adding the standard deviation shadow\n",
    "    plt.fill_between(frame_numbers, radius_data - std_dev_data, radius_data + std_dev_data, alpha=0.2)\n",
    "\n",
    "plt.title('Radius from Center Over Time')\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Mean Radius from Center')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949026fe-ab05-40ae-9d8d-a5850df2b0f4",
   "metadata": {},
   "source": [
    "### Quantification of blobs spread "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab93172-fa62-4d4d-9797-092dba450950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(x_coords, y_coords):\n",
    "    mean_x = np.mean(x_coords)\n",
    "    mean_y = np.mean(y_coords)\n",
    "    std_x = np.std(x_coords)\n",
    "    std_y = np.std(y_coords)\n",
    "    cov_matrix = np.cov(x_coords, y_coords)\n",
    "    return mean_x, mean_y, std_x, std_y, cov_matrix\n",
    "\n",
    "def plot_density(ax, mean_x, mean_y, cov_matrix, x_coords, y_coords, x_range, y_range):\n",
    "    x, y = np.mgrid[x_range[0]:x_range[1]:.1, y_range[0]:y_range[1]:.1]\n",
    "    pos = np.dstack((x, y))\n",
    "    rv = multivariate_normal([mean_x, mean_y], cov_matrix)\n",
    "    z = rv.pdf(pos)\n",
    "    ax.contourf(x, y, z, cmap=\"Blues\")\n",
    "    ax.set_xlim(x_range)\n",
    "    ax.set_ylim(y_range)\n",
    "#     ax.set_title(\"Density Estimation (KDE)\")\n",
    "    ax.set_xlabel(\"x-coordinate\")\n",
    "    ax.set_ylabel(\"y-coordinate\")\n",
    "\n",
    "def plot_scatter(x_coords, y_coords):\n",
    "    plt.scatter(x_coords, y_coords, c='red', label='Blob Centers')\n",
    "    plt.xlabel('x-coordinate')\n",
    "    plt.ylabel('y-coordinate')\n",
    "    plt.title('Scatter Plot of Blob Centers')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is for one frame ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density plot of cm distribution in frame \"frame_chosen\" ###\n",
    "\n",
    "This is mostly for visualization on one frame. The analysis is used on all frames though. \n",
    "\n",
    "This code snippet visualizes the spatial distribution of the blobs center of mass using Kernel Density Estimation (KDE). \n",
    "\n",
    "1. Frame Selection: The script sets frame_chosen to 14, indicating that it's interested in analyzing data corresponding to the 14th frame in each dataset.\n",
    "\n",
    "2. Global Coordinate Extents: It calculates the global minimum and maximum x and y coordinates across all datasets to ensure consistent plotting scales.\n",
    "\n",
    "3. Subplot Arrangement: The number of rows and columns for the subplots is determined based on the square root of the total number of datasets (n_datasets).\n",
    "\n",
    "4. Creating Subplots: A figure with a grid of subplots is created.\n",
    "\n",
    "5. Data Analysis and Plotting:\n",
    "\n",
    "    a. It loops through each dataset.\n",
    "    b. For each dataset, it checks if frame_chosen is within the valid range of frames.\n",
    "    c. Extracts the centers (coordinates) for the chosen frame from the current dataset.\n",
    "    d. Skips datasets with insufficient data.\n",
    "    e. Calculates statistical measures (mean, standard deviation, covariance matrix).\n",
    "    f. Performs a Kernel Density Estimation (KDE) based on these statistics and plots the density on the corresponding subplot.\n",
    "    g. Cleaning Up: The script removes any unused subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb06284-abe4-4aad-8f74-e0adce1d7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Assuming calculate_stats and plot_density are defined functions\n",
    "\n",
    "# Frame chosen for analysis\n",
    "frame_chosen = 14\n",
    "\n",
    "# Assuming all_centers is derived from all_centers_norm_per_hdf5 for the chosen frame\n",
    "all_centers = [dataset[frame_chosen] for dataset in all_centers_norm_per_hdf5 if frame_chosen < len(dataset)]\n",
    "\n",
    "# Determine global min and max for x and y coordinates\n",
    "global_min_x = min([min(centers[:,0]) for centers in all_centers if len(centers) > 0])\n",
    "global_max_x = max([max(centers[:,0]) for centers in all_centers if len(centers) > 0])\n",
    "global_min_y = min([min(centers[:,1]) for centers in all_centers if len(centers) > 0])\n",
    "global_max_y = max([max(centers[:,1]) for centers in all_centers if len(centers) > 0])\n",
    "\n",
    "# Number of datasets\n",
    "n_datasets = len(all_centers)\n",
    "\n",
    "# Number of rows and columns for subplots\n",
    "n_rows = int(np.ceil(np.sqrt(n_datasets)))\n",
    "n_cols = n_rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 15))\n",
    "axes = axes.flatten()  # Flatten the 2D array to 1D for easier indexing\n",
    "\n",
    "# Extract labels for each dataset\n",
    "dataset_labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "for dataset_index, centers in enumerate(all_centers):\n",
    "    # Check for empty lists\n",
    "    if len(centers) == 0:\n",
    "        print(f\"Skipping {dataset_labels[dataset_index]}, Frame {frame_chosen} due to lack of data.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_x, mean_y, std_x, std_y, cov_matrix = calculate_stats(centers[:,0], centers[:,1])\n",
    "\n",
    "    # Try to compute KDE\n",
    "    try:\n",
    "        rv = multivariate_normal([mean_x, mean_y], cov_matrix)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(f\"Could not compute KDE for {dataset_labels[dataset_index]}, Frame {frame_chosen}.\")\n",
    "        continue\n",
    "            \n",
    "    # Plot KDE on the corresponding subplot\n",
    "    plot_density(axes[dataset_index], mean_x, mean_y, cov_matrix, centers[:,0], centers[:,1], (global_min_x, global_max_x), (global_min_y, global_max_y))\n",
    "    \n",
    "    # Add a title with the dataset name and frame number\n",
    "    axes[dataset_index].set_title(f'{dataset_labels[dataset_index]}, Frame {frame_chosen}')\n",
    "\n",
    "    # Remove y-axis label except for the first plot\n",
    "    if dataset_index != 0:\n",
    "        axes[dataset_index].set_ylabel('')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(dataset_index + 1, n_rows * n_cols):\n",
    "    if i < len(axes):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44810d38",
   "metadata": {},
   "source": [
    "## Distance between center of mass blobs ##\n",
    "\n",
    "This code snippet is designed to visualize the distribution of pairwise distances between blobs center of mass within multiple datasets. It uses violin plots to display this information.\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "    a. Initialize a List for Distances: all_distances is initialized to store the pairwise distances for each dataset.\n",
    "\n",
    "    b. Loop Through Datasets: The code iterates over a certain number of datasets (n_datasets), which is determined by the length of all_centers.\n",
    "\n",
    "2. Process Each Dataset:\n",
    "\n",
    "    a. For each dataset, it retrieves a list of centers (centers).\n",
    "\n",
    "    b. It checks if the list of centers is empty. If so, it skips the dataset and appends an empty list to all_distances.\n",
    "    \n",
    "    c. If the list is not empty, it calculates the pairwise distances between all centers using Euclidean distance. This is done using pdist from the SciPy library.\n",
    "\n",
    "    d. These distances are then appended to all_distances.\n",
    "\n",
    "3. Visualization with Violin and Swarm Plots\n",
    "\n",
    "    a. Prepare for Violin Plot:\n",
    "\n",
    "        i. A new figure is created with a specified size.\n",
    "\n",
    "        ii. violin_data is set to all_distances, which now contains lists of pairwise distances for each dataset.\n",
    "\n",
    "        iii. Labels for each dataset are generated in the format Dataset_{i+1}.\n",
    "    \n",
    "    b. Create Violin Plot:\n",
    "\n",
    "        i. A violin plot is created using Seaborn's violinplot. This plot will visualize the distribution of pairwise distances within each dataset.\n",
    "        \n",
    "        ii. The inner=\"quartile\" parameter in the violin plot indicates that the quartiles will be shown inside the violins.\n",
    "        scale=\"count\" scales the width of the violins based on the number of observations in each dataset.\n",
    "\n",
    "    c. Swarm Plot Overlay:\n",
    "\n",
    "        i. A swarm plot is overlaid on the violin plot with sns.swarmplot. This adds individual data points to the visualization, enhancing the interpretability of the distribution.\n",
    "\n",
    "        ii. Customization and Display:\n",
    "\n",
    "        iii. The plot is titled 'Violin Plots for Pairwise Distances Between Blobs'.\n",
    "        iv. Axes are labeled appropriately, and x-axis ticks are set to the generated dataset labels.\n",
    "        v. The plot is displayed using plt.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aeaa9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to store distances for all datasets\n",
    "# all_distances = []\n",
    "\n",
    "# # Extract dataset labels\n",
    "# dataset_labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]  # Replace with actual paths\n",
    "\n",
    "# for dataset_index, centers in enumerate(all_centers_norm_per_hdf5):\n",
    "#     # Check for empty lists\n",
    "#     if len(centers) == 0:\n",
    "#         print(f\"Skipping {dataset_labels[dataset_index]} due to lack of data.\")\n",
    "#         all_distances.append([])  # Append an empty list for this dataset\n",
    "#         continue\n",
    "\n",
    "#     # Flatten the centers data for each frame and calculate pairwise distances\n",
    "#     flattened_centers = np.vstack(centers)\n",
    "#     distances = pdist(flattened_centers, 'euclidean')\n",
    "    \n",
    "#     # Append the distances to the all_distances list\n",
    "#     all_distances.append(distances)\n",
    "\n",
    "# # Create a new figure for the violin plot\n",
    "# plt.figure(figsize=(15, 8))\n",
    "\n",
    "# # Prepare data for violin plot\n",
    "# violin_data = all_distances  # Each sublist in 'all_distances' will become a violin in the violin plot\n",
    "\n",
    "# # Create violin plot using seaborn\n",
    "# sns.violinplot(data=violin_data, inner=\"quartile\", scale=\"count\")\n",
    "\n",
    "# # Add individual data points as a swarmplot\n",
    "# sns.swarmplot(data=violin_data, color=\"k\", alpha=0.5)\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.title('Pairwise Distances Between Blobs')\n",
    "# plt.xlabel('Datasets')\n",
    "# plt.ylabel('Pairwise Distance')\n",
    "# plt.xticks(ticks=range(len(dataset_labels)), labels=dataset_labels)  # Set x-axis labels to actual dataset names\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Assuming all_centers_norm_per_hdf5 and hdf5_file_paths are defined\n",
    "\n",
    "# Initialize an empty list to store distances for a subset of datasets\n",
    "all_distances = []\n",
    "\n",
    "# Extract dataset labels\n",
    "dataset_labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "# Limit the number of datasets to process for testing\n",
    "num_datasets_to_test = 5  # Adjust this number as needed\n",
    "\n",
    "for dataset_index, centers in enumerate(all_centers_norm_per_hdf5[:num_datasets_to_test]):\n",
    "    if len(centers) == 0:\n",
    "        print(f\"Skipping {dataset_labels[dataset_index]} due to lack of data.\")\n",
    "        continue\n",
    "\n",
    "    flattened_centers = np.vstack(centers)\n",
    "    distances = pdist(flattened_centers, 'euclidean')\n",
    "    all_distances.append(distances)\n",
    "\n",
    "# Create violin plot for the subset\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.violinplot(data=all_distances, inner=\"quartile\", scale=\"count\")\n",
    "sns.swarmplot(data=all_distances, color=\"k\", alpha=0.5)\n",
    "plt.title('Pairwise Distances Between Blobs (Subset)')\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('Pairwise Distance')\n",
    "plt.xticks(ticks=range(len(dataset_labels[:num_datasets_to_test])), labels=dataset_labels[:num_datasets_to_test])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b0364",
   "metadata": {},
   "source": [
    "# Size blob  #\n",
    "Estimation of Major and Mionor axis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for one frame ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77074ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store areas for each sublist\n",
    "areas = []\n",
    "\n",
    "# Specify the frame index you want to analyze for each HDF5 file\n",
    "frame_i = 3  # Change this to the frame index you want to analyze\n",
    "\n",
    "# Extract labels for each dataset\n",
    "dataset_labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "# Iterate through each sublist corresponding to an HDF5 file\n",
    "for sublist in all_images:\n",
    "    sublist_areas = []  # Initialize a sublist-specific area list\n",
    "    \n",
    "    if frame_i < len(sublist):\n",
    "        # Assuming you have the labeled image 'label_image' for the chosen frame\n",
    "        label_image = label(sublist[frame_i] > threshold_otsu(sublist[frame_i]))\n",
    "\n",
    "        # Calculate the region properties\n",
    "        stats = regionprops(label_image)\n",
    "\n",
    "        # Calculate the areas for each blob and append to the sublist-specific area list\n",
    "        sublist_areas.extend(np.pi * np.array([prop.major_axis_length * prop.minor_axis_length / 4 for prop in stats]))\n",
    "    \n",
    "    # Append the sublist-specific area list to the main 'areas' list\n",
    "    areas.append(sublist_areas)\n",
    "\n",
    "# Create a new figure for the violin plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Prepare data for violin plot\n",
    "violin_data = areas  # Each sublist in 'areas' will become a violin in the violin plot\n",
    "\n",
    "# Create violin plot using seaborn\n",
    "sns.violinplot(data=violin_data, inner=\"quartile\", scale=\"count\")\n",
    "\n",
    "# Add individual data points as a swarmplot\n",
    "sns.swarmplot(data=violin_data, color=\"k\", alpha=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Violin Plots for Blob Areas')\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('Blob Area (pixels^2/9)')\n",
    "plt.xticks(ticks=range(len(dataset_labels)), labels=dataset_labels)  # Set x-axis labels to actual dataset names\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On all frames ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store areas for each sublist\n",
    "areas = []\n",
    "\n",
    "# Extract labels for each dataset\n",
    "dataset_labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "# Iterate through each sublist corresponding to an HDF5 file\n",
    "for sublist in all_images:\n",
    "    sublist_areas = []  # Initialize a sublist-specific area list\n",
    "\n",
    "    # Iterate through each frame in the sublist\n",
    "    for frame in sublist:\n",
    "        # Label the image for the current frame\n",
    "        label_image = label(frame > threshold_otsu(frame))\n",
    "\n",
    "        # Calculate the region properties\n",
    "        stats = regionprops(label_image)\n",
    "\n",
    "        # Calculate the areas for each blob and append to the sublist-specific area list\n",
    "        sublist_areas.extend(np.pi * np.array([prop.major_axis_length * prop.minor_axis_length / 4 for prop in stats]))\n",
    "    \n",
    "    # Append the sublist-specific area list to the main 'areas' list\n",
    "    areas.append(sublist_areas)\n",
    "\n",
    "# Create a new figure for the violin plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Prepare data for violin plot\n",
    "violin_data = areas  # Each sublist in 'areas' will become a violin in the violin plot\n",
    "\n",
    "# Create violin plot using seaborn\n",
    "sns.violinplot(data=violin_data, inner=\"quartile\", scale=\"count\")\n",
    "\n",
    "# Add individual data points as a swarmplot\n",
    "sns.swarmplot(data=violin_data, color=\"k\", alpha=0.5)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Violin Plots for Blob Areas Across All Frames')\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('Blob Area (pixels^2)')\n",
    "plt.xticks(ticks=range(len(dataset_labels)), labels=dataset_labels)  # Set x-axis labels to actual dataset names\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series plot ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store mean and standard deviation of areas for each frame in each sublist\n",
    "mean_areas_per_frame = []\n",
    "std_areas_per_frame = []\n",
    "\n",
    "# Extract labels for each dataset\n",
    "dataset_labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "# Iterate through each sublist corresponding to an HDF5 file\n",
    "for sublist in all_images:\n",
    "    frame_areas = []  # Store areas for each frame in the sublist\n",
    "\n",
    "    # Iterate through each frame in the sublist\n",
    "    for frame in sublist:\n",
    "        # Label the image for the current frame\n",
    "        label_image = label(frame > threshold_otsu(frame))\n",
    "\n",
    "        # Calculate the region properties\n",
    "        stats = regionprops(label_image)\n",
    "\n",
    "        # Calculate the areas for each blob\n",
    "        areas = [np.pi * prop.major_axis_length * prop.minor_axis_length / 4 for prop in stats] if stats else [0]\n",
    "\n",
    "        # Append the areas for this frame to frame_areas\n",
    "        frame_areas.append(areas)\n",
    "\n",
    "    # Calculate and append mean and standard deviation of areas per frame\n",
    "    mean_areas = [np.mean(areas) for areas in frame_areas]\n",
    "    std_areas = [np.std(areas) for areas in frame_areas]\n",
    "    mean_areas_per_frame.append(mean_areas)\n",
    "    std_areas_per_frame.append(std_areas)\n",
    "\n",
    "# Create a new figure for the time series plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plotting time series with standard deviation for each dataset\n",
    "for idx, (mean_areas, std_areas) in enumerate(zip(mean_areas_per_frame, std_areas_per_frame)):\n",
    "    frames = np.arange(len(mean_areas))\n",
    "    plt.plot(frames, mean_areas, label=dataset_labels[idx] if idx < len(dataset_labels) else f\"Dataset {idx + 1}\")\n",
    "    plt.fill_between(frames, np.array(mean_areas) - np.array(std_areas), np.array(mean_areas) + np.array(std_areas), alpha=0.3)\n",
    "\n",
    "plt.title('Blob Areas')\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Mean Blob Area (pixels^2)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68085441",
   "metadata": {},
   "source": [
    "# Geometric characterization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea411259-18e7-47a3-a252-42de41477bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74154487",
   "metadata": {},
   "source": [
    "# Hu Moments #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c40147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_generate(i, j, image):\n",
    "    _lambda = (i + j) / 2 + 1\n",
    "    mu_ij = mu_generate(i, j, image)\n",
    "    mu_00 = np.sum(image)\n",
    "    eta_ij = mu_ij / mu_00  # ** _lambda\n",
    "    return eta_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_generate(i, j, image):\n",
    "    M = image.shape[0]\n",
    "    N = image.shape[1]\n",
    "    m_00 = np.sum(image)\n",
    "    x = np.arange(1, M+1)\n",
    "    y = np.arange(1, N+1)\n",
    "    m_10 = np.sum(x * image)\n",
    "    m_01 = np.sum(y * image)\n",
    "\n",
    "    x_prime = m_10 / m_00\n",
    "    y_prime = m_01 / m_00\n",
    "\n",
    "    x_component = (x - x_prime) ** i\n",
    "    y_component = (y - y_prime) ** j\n",
    "\n",
    "    product_xy_component = np.outer(x_component, y_component)\n",
    "    product_xy_component_image = product_xy_component * image\n",
    "\n",
    "    mu_ij = np.sum(product_xy_component_image)\n",
    "    return mu_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2double(image):\n",
    "    return image.astype(np.float64) / np.max(image)\n",
    "    # normalizes the image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7f2b6-ff97-4d10-b670-6df64caf5234",
   "metadata": {},
   "source": [
    "Compute Hu moments for one frame for each video and plot them together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4991b-dc86-4b77-93c3-cc2de5b6caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the number of videos (HDF5 files)\n",
    "num_videos = len(all_images)\n",
    "\n",
    "# Initialize an empty array to store the Hu moments for all videos\n",
    "phi_1 = np.zeros((num_videos, 7))\n",
    "\n",
    "# Choose a specific frame for analysis (e.g., frame 3)\n",
    "frame_i = 3\n",
    "\n",
    "# Loop through each video (HDF5 file)\n",
    "for video_i, sublist in enumerate(all_images):\n",
    "    if frame_i < len(sublist):\n",
    "        # Assuming you have the labeled image 'label_image' for the chosen frame\n",
    "        im = sublist[frame_i]\n",
    "\n",
    "        eta_02 = eta_generate(0, 2, im)\n",
    "        eta_03 = eta_generate(0, 3, im)\n",
    "        eta_11 = eta_generate(1, 1, im)\n",
    "        eta_12 = eta_generate(1, 2, im)\n",
    "        eta_20 = eta_generate(2, 0, im)\n",
    "        eta_21 = eta_generate(2, 1, im)\n",
    "        eta_30 = eta_generate(3, 0, im)\n",
    "\n",
    "        phi_1[video_i, 0] = eta_20 + eta_02\n",
    "        phi_1[video_i, 1] = (eta_20 + eta_02) ** 2 + 4 * eta_11 ** 2\n",
    "        phi_1[video_i, 2] = (eta_30 - 3 * eta_12) ** 2 + (3 * eta_21 - eta_03) ** 2\n",
    "        phi_1[video_i, 3] = (eta_30 + eta_12) ** 2 + (eta_21 + eta_03) ** 2\n",
    "        phi_1[video_i, 4] = (3 * eta_21 - eta_03) * (eta_21 + eta_03) * (\n",
    "                    (eta_30 + eta_12) ** 2 - (eta_21 + eta_03) ** 2) + (\n",
    "                                       eta_30 - 3 * eta_12) * (eta_30 + eta_12) * (\n",
    "                                               (eta_30 + eta_12) ** 2 - 3 * (eta_21 + eta_03) ** 2)\n",
    "        phi_1[video_i, 5] = (3 * eta_21 - eta_03) * ((eta_30 + eta_12) ** 2 - (eta_21 + eta_03) ** 2) + 4 * eta_11 * (\n",
    "                    eta_30 + eta_12) * (eta_21 + eta_03)\n",
    "        phi_1[video_i, 6] = (3 * eta_21 - eta_03) * (eta_30 + eta_12) * (\n",
    "                    (eta_30 + eta_12) ** 2 - 3 * (eta_21 + eta_03) ** 2) - (\n",
    "                                       eta_30 - 3 * eta_12) * (eta_21 + eta_03) * (\n",
    "                                               3 * (eta_30 + eta_12) ** 2 - (eta_21 + eta_03) ** 2)\n",
    "\n",
    "# Plot the Hu moments for all videos\n",
    "plt.figure()\n",
    "for video_i in range(num_videos):\n",
    "    file_path = hdf5_file_paths[video_i]\n",
    "    # Get the label for this file using the function\n",
    "    label_file = extract_label_from_path(file_path)\n",
    "    plt.plot(phi_1[video_i], '.-', label=f'{label_file}')\n",
    "plt.title('Hu Moments for frame i', fontsize=14)\n",
    "plt.xlabel('Hu Moment #', fontsize=12)\n",
    "plt.ylabel(r'$\\phi$', fontsize=16)\n",
    "plt.axis('tight')\n",
    "\n",
    "# Place the legend outside of the plot\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Adjust the layout to make room for the legend\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b704db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the number of videos (HDF5 files)\n",
    "num_videos = len(all_images)\n",
    "\n",
    "# Initialize the number of frames per video (assuming all videos have the same number of frames)\n",
    "num_frames_per_video = len(all_images[0])\n",
    "\n",
    "# Initialize arrays to store Hu moments and distances for all videos and frames\n",
    "phi = np.zeros((num_videos, num_frames_per_video, 7))\n",
    "dist = np.zeros((num_videos, num_frames_per_video, 1))\n",
    "\n",
    "# Loop through each video\n",
    "for video_i, video_frames in enumerate(all_images):\n",
    "    for frame_i, im in enumerate(video_frames):\n",
    "        eta_02 = eta_generate(0, 2, im)\n",
    "        eta_03 = eta_generate(0, 3, im)\n",
    "        eta_11 = eta_generate(1, 1, im)\n",
    "        eta_12 = eta_generate(1, 2, im)\n",
    "        eta_20 = eta_generate(2, 0, im)\n",
    "        eta_21 = eta_generate(2, 1, im)\n",
    "        eta_30 = eta_generate(3, 0, im)\n",
    "\n",
    "        phi[video_i, frame_i, 0] = eta_20 + eta_02\n",
    "        phi[video_i, frame_i, 1] = (eta_20 + eta_02) ** 2 + 4 * eta_11 ** 2\n",
    "        phi[video_i, frame_i, 2] = (eta_30 - 3 * eta_12) ** 2 + (3 * eta_21 - eta_03) ** 2\n",
    "        phi[video_i, frame_i, 3] = (eta_30 + eta_12) ** 2 + (eta_21 + eta_03) ** 2\n",
    "        phi[video_i, frame_i, 4] = (3 * eta_21 - eta_03) * (eta_21 + eta_03) * (\n",
    "                (eta_30 + eta_12) ** 2 - (eta_21 + eta_03) ** 2) + (\n",
    "                                           eta_30 - 3 * eta_12) * (eta_30 + eta_12) * (\n",
    "                                                   (eta_30 + eta_12) ** 2 - 3 * (eta_21 + eta_03) ** 2)\n",
    "        phi[video_i, frame_i, 5] = (3 * eta_21 - eta_03) * ((eta_30 + eta_12) ** 2 - (eta_21 + eta_03) ** 2) + 4 * eta_11 * (\n",
    "                eta_30 + eta_12) * (eta_21 + eta_03)\n",
    "        phi[video_i, frame_i, 6] = (3 * eta_21 - eta_03) * (eta_30 + eta_12) * (\n",
    "                (eta_30 + eta_12) ** 2 - 3 * (eta_21 + eta_03) ** 2) - (\n",
    "                                           eta_30 - 3 * eta_12) * (eta_21 + eta_03) * (\n",
    "                                                   3 * (eta_30 + eta_12) ** 2 - (eta_21 + eta_03) ** 2)\n",
    "\n",
    "        dist[video_i, frame_i, 0] = np.sqrt(np.sum(phi[video_i, frame_i, :] ** 2))\n",
    "\n",
    "\n",
    "# Plot Hu moments for all videos in time\n",
    "plt.figure(figsize=(12, 6))\n",
    "for video_i in range(num_videos):\n",
    "    for moment_i in range(7):\n",
    "        file_path = hdf5_file_paths[video_i]\n",
    "        # Get the label for this file using the function\n",
    "        label_file = extract_label_from_path(file_path)\n",
    "        plt.plot(phi[video_i, :, moment_i], '.-', label=f'{label_file}, Hu Moment {moment_i + 1}')\n",
    "plt.title('Hu Moments Over Time', fontsize=14)\n",
    "plt.xlabel('Frame Index', fontsize=12)\n",
    "plt.ylabel(r'$\\phi$', fontsize=16)\n",
    "plt.axis('tight')\n",
    "# Position the legend outside of the plot\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=3)\n",
    "plt.show()\n",
    "\n",
    "# Plot distance for all videos in time\n",
    "plt.figure(figsize=(12, 6))\n",
    "for video_i in range(num_videos):\n",
    "    file_path = hdf5_file_paths[video_i]\n",
    "    # Get the label for this file using the function\n",
    "    label_file = extract_label_from_path(file_path)\n",
    "    plt.plot(dist[video_i, :, 0], '.-', label=f'{label_file}')\n",
    "plt.title('Distance Over Time', fontsize=14)\n",
    "plt.xlabel('Frame Index', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "plt.axis('tight')\n",
    "# Position the legend outside of the plot\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bb5e7-dd02-41a6-b8bb-9d1f99309f2b",
   "metadata": {},
   "source": [
    "What $\\textbf{dist}$ represents: \n",
    "\n",
    "In simpler terms, the dist value gives a measure of the \"magnitude\" or \"size\" of the Hu moments for each frame (7 dimensions in this case since there are seven Hu moments). This can be thought of as a single value summary of the shape described by the Hu moments. In the context of the code, this could be used to quickly compare or assess the shapes in each frame across videos. If two frames have similar dist values, their shapes (as described by the Hu moments) could be considered to be of similar complexity or magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfefad6e",
   "metadata": {},
   "source": [
    "Perform Hierarchical clustering on the images based on the distance estimated through the Hu moments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290fe7d",
   "metadata": {},
   "source": [
    "Plot the normalized Hu moments in time. \n",
    "\n",
    "Normalization is useful because the value range of the different coefficient varies much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape phi to combine the video and frame dimensions\n",
    "phi_reshaped = phi.reshape(-1, phi.shape[-1])\n",
    "\n",
    "# Compute the zscore for the reshaped phi\n",
    "phi_normalized_reshaped = zscore(phi_reshaped, axis=0)\n",
    "\n",
    "# Reshape it back to the original 3D structure\n",
    "phi_normalized = phi_normalized_reshaped.reshape(phi.shape)\n",
    "\n",
    "\n",
    "# Plot normalized Hu moments for each video\n",
    "plt.figure(figsize=(12, 6))\n",
    "for video_i in range(phi_normalized.shape[0]):\n",
    "    file_path = hdf5_file_paths[video_i]\n",
    "    # Get the label for this file using the function\n",
    "    label_file = extract_label_from_path(file_path)\n",
    "    for moment_i in range(7):\n",
    "        plt.plot(phi_normalized[video_i, :, moment_i], '.-', label=f'{label_file}, Hu Moment {moment_i + 1}')\n",
    "plt.title('Normalized Hu Moments Over Time', fontsize=14)\n",
    "plt.ylabel(r'$\\phi$', fontsize=16)\n",
    "plt.xlabel('t (frames)', fontsize=12)\n",
    "plt.axis('tight')\n",
    "\n",
    "# Place the legend to the right of the plot\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()  # Adjusts the plot so that everything fits neatly\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Assuming you want to compute pairwise distances between frames for a specific video\n",
    "video_index = 0  # e.g., for the first video\n",
    "distance_vector = pdist(phi_normalized[video_index], 'minkowski')\n",
    "distance_matrix = squareform(distance_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af268e",
   "metadata": {},
   "source": [
    "# Zernike Moments #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb49c9-ab03-4ae5-aa21-17ab64c0a7cc",
   "metadata": {},
   "source": [
    "Come back to this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1631c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zernike_moments(image, radius, degree):\n",
    "    # Compute the Zernike moments for a given image, radius, and degree\n",
    "    \n",
    "    # Create a grid of points within the image\n",
    "    rows, cols = image.shape\n",
    "    x, y = np.meshgrid(np.arange(cols), np.arange(rows))\n",
    "    points = np.stack((x, y), axis=2)\n",
    "    \n",
    "    # Compute the centroid of the image\n",
    "    centroid = measure.regionprops(image.astype(int))[0].centroid\n",
    "    \n",
    "    # Compute the distance of each point from the centroid\n",
    "    distances = np.sqrt((points[:, :, 0] - centroid[1])**2 + (points[:, :, 1] - centroid[0])**2)\n",
    "    \n",
    "    # Create a binary mask for points within the specified radius\n",
    "    mask = distances <= radius\n",
    "    \n",
    "    # Compute the Zernike moments using the binary mask\n",
    "    moments = np.zeros(degree + 1, dtype=np.complex128)\n",
    "    \n",
    "    for n in range(degree + 1):\n",
    "        for m in range(-n, n + 1, 2):\n",
    "            re = np.sum(image * draw.disk(centroid[::-1], radius, shape=image.shape) * np.real(zernike(n, m, distances/radius) * mask))\n",
    "            im = np.sum(image * draw.disk(centroid[::-1], radius, shape=image.shape) * np.imag(zernike(n, m, distances/radius) * mask))\n",
    "            moments[n] += re + 1j * im\n",
    "    \n",
    "    return moments\n",
    "\n",
    "def zernike(n, m, r):\n",
    "    # Compute the Zernike polynomials for a given degree and order\n",
    "    \n",
    "    if m > 0:\n",
    "        return np.sqrt((2 * (n + 1)) / np.pi) * np.polynomial.chebyshev.chebval(r, np.polynomial.zernike.zernike_poly(n, m))\n",
    "    elif m == 0:\n",
    "        return np.sqrt(n + 1) * np.pi / 2\n",
    "    else:\n",
    "        return np.sqrt((2 * (n + 1)) / np.pi) * np.polynomial.chebyshev.chebval(r, np.polynomial.zernike.zernike_poly(n, -m)) * 1j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zernike_moments(images[image_i], 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Transform #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f664d",
   "metadata": {},
   "source": [
    "### Wavelet Transform implementation ###\n",
    "\n",
    "This code performs wavelet transform analysis on video frames and then visualizes and compares the frames using the results of the transform. \n",
    "\n",
    "1. Wavelet Transform Function (apply_wavelet_transform):\n",
    "\n",
    "    a. This function applies a discrete wavelet transform to an image using the Haar wavelet.\n",
    "    b. It returns the coefficients of the transform, which include the approximation coefficients (cA) and the detailed coefficients in horizontal (cH), vertical (cV), and diagonal (cD) orientations.\n",
    "\n",
    "2. Plotting Wavelet Coefficients (plot_wavelet_coeffs):\n",
    "\n",
    "    a. This function visualizes the wavelet coefficients of a frame.\n",
    "    b. It plots the approximation and detailed coefficients in separate subplots.\n",
    "    c. The coefficients are displayed in grayscale, and each subplot is titled according to the type of coefficient it represents (approximation, horizontal, vertical, diagonal).\n",
    "\n",
    "3. Processing and Visualizing Each Video Frame:\n",
    "\n",
    "    a. The code iterates through a list of videos (all_images). For each video, it selects a specific frame (defined by frame_index).\n",
    "I   b. It applies the wavelet transform to this frame and plots the resulting coefficients using the plot_wavelet_coeffs function.\n",
    "\n",
    "4. Storing Wavelet Coefficients:\n",
    "\n",
    "    a. For each video, the wavelet coefficients of the specified frame are computed and stored in a list (all_coeffs).\n",
    "\n",
    "5. Computing Similarity Matrix:\n",
    "\n",
    "    a. A similarity matrix is constructed to compare frames across videos.\n",
    "    b.  The Euclidean distance between the flattened approximation coefficients (cA) of each pair of frames is calculated. This distance serves as a measure of similarity between frames.\n",
    "    c. The computed distances are stored in the similarity matrix.\n",
    "\n",
    "6. Visualizing the Similarity Matrix:\n",
    "\n",
    "    a. A heatmap is created using Seaborn to visualize the similarity matrix.\n",
    "    b. The heatmap displays the Euclidean distances between the approximation coefficients of the frames, providing a visual representation of frame similarity across different videos.\n",
    "    c. The axes of the heatmap are labeled with the video numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ce874",
   "metadata": {},
   "source": [
    "### Compute wavelet transform on all frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa724ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_wavelet_transform(image):\n",
    "    # Apply wavelet transform to the image and return coefficients\n",
    "    coeffs = pywt.dwt2(image, 'haar')\n",
    "    return coeffs\n",
    "\n",
    "# Assuming 'all_videos' is a list of videos, where each video is a list of frames\n",
    "num_videos = len(all_images)\n",
    "\n",
    "# Dictionaries to store coefficients for each video\n",
    "cA_dict = {}\n",
    "cH_dict = {}\n",
    "cV_dict = {}\n",
    "cD_dict = {}\n",
    "\n",
    "for vid_index, video_frames in enumerate(all_images):\n",
    "    # Lists to store coefficients for each frame in the current video\n",
    "    cA_list = []\n",
    "    cH_list = []\n",
    "    cV_list = []\n",
    "    cD_list = []\n",
    "\n",
    "    for frame in video_frames:\n",
    "        # Apply wavelet transform\n",
    "        coeffs = apply_wavelet_transform(frame)\n",
    "\n",
    "        # Get the approximation and details coefficients\n",
    "        cA, (cH, cV, cD) = coeffs\n",
    "\n",
    "        # Append the coefficients for each frame to the respective lists\n",
    "        cA_list.append(cA)\n",
    "        cH_list.append(cH)\n",
    "        cV_list.append(cV)\n",
    "        cD_list.append(cD)\n",
    "    \n",
    "    # Convert the lists to numpy arrays for further processing if needed and store in the dictionaries\n",
    "    cA_dict[vid_index] = np.array(cA_list)\n",
    "    cH_dict[vid_index] = np.array(cH_list)\n",
    "    cV_dict[vid_index] = np.array(cV_list)\n",
    "    cD_dict[vid_index] = np.array(cD_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208ceb5",
   "metadata": {},
   "source": [
    "### Don't run this on the divergent data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61225b4-fb3a-4820-a537-75718a10b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wavelet_transform(image):\n",
    "    # Apply wavelet transform to the image and return coefficients\n",
    "    coeffs = pywt.dwt2(image, 'haar')\n",
    "    return coeffs\n",
    "\n",
    "# Assuming 'all_videos' is a list of videos, where each video is a list of frames\n",
    "num_videos = len(all_images)\n",
    "\n",
    "# Extract dataset labels\n",
    "dataset_labels = [extract_label_from_path(file_path) for file_path in hdf5_file_paths]\n",
    "\n",
    "# Dictionaries to store coefficients for each video\n",
    "cA_dict = {}\n",
    "cH_dict = {}\n",
    "cV_dict = {}\n",
    "cD_dict = {}\n",
    "\n",
    "for vid_index, video_frames in enumerate(all_images):\n",
    "    # Lists to store coefficients for each frame in the current video\n",
    "    cA_list, cH_list, cV_list, cD_list = [], [], [], []\n",
    "\n",
    "    for frame in video_frames:\n",
    "        coeffs = apply_wavelet_transform(frame)\n",
    "        cA, (cH, cV, cD) = coeffs\n",
    "        cA_list.append(cA)\n",
    "        cH_list.append(cH)\n",
    "        cV_list.append(cV)\n",
    "        cD_list.append(cD)\n",
    "    \n",
    "    cA_dict[vid_index] = np.array(cA_list)\n",
    "    cH_dict[vid_index] = np.array(cH_list)\n",
    "    cV_dict[vid_index] = np.array(cV_list)\n",
    "    cD_dict[vid_index] = np.array(cD_list)\n",
    "\n",
    "# Variable to control plot display\n",
    "display_plots = True\n",
    "\n",
    "if display_plots:\n",
    "    num_frames = cA_dict[0].shape[0]  # Assuming each video has the same number of frames\n",
    "    fig, axes = plt.subplots(num_frames * num_videos, 4, figsize=(12, 4*num_frames*num_videos))\n",
    "\n",
    "    for vid_index in range(num_videos):\n",
    "        for k in range(num_frames):\n",
    "            row_idx = vid_index * num_frames + k\n",
    "            dataset_label = dataset_labels[vid_index]  # Use the dataset label\n",
    "\n",
    "            axes[row_idx, 0].imshow(cA_dict[vid_index][k], cmap='viridis', aspect='auto', origin='upper')\n",
    "            axes[row_idx, 0].set_title(f'{dataset_label} - Approximation')\n",
    "            \n",
    "            axes[row_idx, 1].imshow(cH_dict[vid_index][k], cmap='viridis', aspect='auto', origin='upper')\n",
    "            axes[row_idx, 1].set_title(f'{dataset_label} - Horizontal Detail')\n",
    "            \n",
    "            axes[row_idx, 2].imshow(cV_dict[vid_index][k], cmap='viridis', aspect='auto', origin='upper')\n",
    "            axes[row_idx, 2].set_title(f'{dataset_label} - Vertical Detail')\n",
    "            \n",
    "            axes[row_idx, 3].imshow(cD_dict[vid_index][k], cmap='viridis', aspect='auto', origin='upper')\n",
    "            axes[row_idx, 3].set_title(f'{dataset_label} - Diagonal Detail')\n",
    "\n",
    "    for ax in axes[0, :]:\n",
    "        ax.set_xlabel('X-Axis')\n",
    "    for ax in axes[:, 0]:\n",
    "        ax.set_ylabel('Frame Index')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be52723",
   "metadata": {},
   "source": [
    "## Distance matrix ##\n",
    "\n",
    "The provided code performs the computation of distance matrices for a set of videos based on wavelet transform coefficients. xtracts wavelet coefficients from video frames, flattens and combines them into a feature matrix, and computes a distance matrix representing the pairwise Euclidean distances between frames based on these features.\n",
    "\n",
    "1. Setup and Initialization:\n",
    "\n",
    "    a. video_keys: This retrieves keys from the cA_dict, which likely represent different videos.\n",
    "    b. distance_matrices: A dictionary initialized to store the distance matrices for each video.\n",
    "\n",
    "2. Iterating Through Videos:\n",
    "\n",
    "    The code iterates over each video, identified by vid_key in video_keys.\n",
    "\n",
    "3. Retrieving and Reshaping Wavelet Coefficients:\n",
    "\n",
    "    a. For each video, it retrieves the wavelet coefficients stored in cA_dict, cH_dict, cV_dict, and cD_dict. These coefficients are likely the approximation, horizontal detail, vertical detail, and diagonal detail coefficients, respectively.\n",
    "    \n",
    "    b. num_frames: The number of frames in each video is determined.\n",
    "    \n",
    "    c. The coefficients for each type are reshaped into 2D arrays where each row represents a frame, and the columns represent the flattened coefficients.\n",
    "\n",
    "4. Creating Feature Matrices:\n",
    "\n",
    "    a. The reshaped (flattened) coefficients are combined into a single feature matrix for each video. This matrix concatenates the approximation, horizontal, vertical, and diagonal coefficients side-by-side.\n",
    "\n",
    "5. Computing Distance Matrices:\n",
    "\n",
    "    a. A distance matrix is calculated using the Euclidean distance metric for each video. This matrix measures the pairwise distances between the feature representations of each frame.\n",
    "    \n",
    "    b. The cdist function from SciPy (assuming imported) is used to calculate the pairwise Euclidean distances between the rows (frames) of the feature matrix.\n",
    "\n",
    "6. Storing Distance Matrices:\n",
    "\n",
    "    a. The computed distance matrix for each video is stored in the distance_matrices dictionary with the vid_key as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_keys = cA_dict.keys()\n",
    "\n",
    "distance_matrices = {}  # Dictionary to store distance matrices for each video\n",
    "\n",
    "for vid_key in video_keys:\n",
    "    cA_array = cA_dict[vid_key]\n",
    "    cH_array = cH_dict[vid_key]\n",
    "    cV_array = cV_dict[vid_key]\n",
    "    cD_array = cD_dict[vid_key]\n",
    "    \n",
    "    num_frames = cA_array.shape[0]\n",
    "\n",
    "    # Reshape the coefficient arrays to have shape (num_frames, height*width)\n",
    "    cA_flat = cA_array.reshape(num_frames, -1)\n",
    "    cH_flat = cH_array.reshape(num_frames, -1)\n",
    "    cV_flat = cV_array.reshape(num_frames, -1)\n",
    "    cD_flat = cD_array.reshape(num_frames, -1)\n",
    "\n",
    "    # Combine the flattened coefficient arrays into a single feature matrix\n",
    "    feature_matrix = np.hstack((cA_flat, cH_flat, cV_flat, cD_flat))\n",
    "\n",
    "    # Calculate the Euclidean distance matrix between frames based on their features\n",
    "    distance_matrix = cdist(feature_matrix, feature_matrix, metric='euclidean')\n",
    "    \n",
    "    # Store the distance matrix for this video\n",
    "    distance_matrices[vid_key] = distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5794bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming distance_matrices and extract_label_from_path are defined\n",
    "\n",
    "for vid_key, distance_matrix in distance_matrices.items():\n",
    "    # Get the label for this dataset\n",
    "    label_file = extract_label_from_path(hdf5_file_paths[vid_key])\n",
    "\n",
    "    # Create a masked array where zeros are masked\n",
    "    masked_distance_matrix = np.ma.masked_where(distance_matrix == 0, distance_matrix)\n",
    "\n",
    "    # Create a new figure for each dataset\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Define a colormap that sets masked values (zeros) to white\n",
    "    cmap = plt.cm.viridis\n",
    "    cmap.set_bad(color='white')\n",
    "\n",
    "    plt.imshow(masked_distance_matrix, cmap=cmap, origin='lower', aspect='auto')\n",
    "    plt.colorbar(label='Euclidean Distance')\n",
    "    plt.title(f'Distance Matrix Heatmap for {label_file}')\n",
    "    plt.xlabel('Frame Index')\n",
    "    plt.ylabel('Frame Index')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e71c2",
   "metadata": {},
   "source": [
    "### Hierarchical clustering ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_keys = distance_matrices.keys()\n",
    "\n",
    "# Iterate over each video key and apply hierarchical clustering\n",
    "for vid_key in video_keys:\n",
    "    \n",
    "    condensed_dist_matrix = pdist(distance_matrices[vid_key])\n",
    "    # Perform hierarchical clustering using the linkage function\n",
    "    Z = linkage(condensed_dist_matrix, method='ward')\n",
    "\n",
    "\n",
    "    # Plot a dendrogram to visualize the clustering\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(Z, labels=np.arange(distance_matrix.shape[0]), leaf_rotation=90, leaf_font_size=8)\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram for Video {vid_key}')\n",
    "    plt.xlabel('Frame Index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mean clustering ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_keys = cA_dict.keys()  # Assuming cA_dict and other related dicts are defined\n",
    "\n",
    "for vid_key in video_keys:\n",
    "    num_clusters = 5  # Assuming the number of clusters (K) is known; set it as needed\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "    cA_array = cA_dict[vid_key]\n",
    "    cH_array = cH_dict[vid_key]\n",
    "    cV_array = cV_dict[vid_key]\n",
    "    cD_array = cD_dict[vid_key]\n",
    "    \n",
    "    num_frames = cA_array.shape[0]\n",
    "    cA_flat = cA_array.reshape(num_frames, -1)\n",
    "    cH_flat = cH_array.reshape(num_frames, -1)\n",
    "    cV_flat = cV_array.reshape(num_frames, -1)\n",
    "    cD_flat = cD_array.reshape(num_frames, -1)\n",
    "    feature_matrix = np.hstack((cA_flat, cH_flat, cV_flat, cD_flat))\n",
    "\n",
    "    labels = kmeans.fit_predict(feature_matrix)  # Fit and predict in one step\n",
    "\n",
    "    # Plotting the clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for cluster_index in range(num_clusters):\n",
    "        cluster_frames = np.arange(len(labels))[labels == cluster_index]\n",
    "        plt.scatter(cluster_frames, \n",
    "                    distance_matrices[vid_key][cluster_frames, 0],\n",
    "                    label=f'Cluster {cluster_index + 1}')\n",
    "\n",
    "    plt.title(f'K-Means Clustering for Video {vid_key}')\n",
    "    plt.xlabel('Frame Index')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "for vid_key in video_keys:\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust eps and min_samples as needed\n",
    "\n",
    "    labels = dbscan.fit_predict(feature_matrix)  # Use the same feature_matrix as in K-means\n",
    "\n",
    "    # Plotting the clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(np.arange(len(labels)), distance_matrices[vid_key][:, 0], c=labels, cmap='viridis')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.title(f'DBSCAN Clustering for Video {vid_key}')\n",
    "    plt.xlabel('Frame Index')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "for vid_key in video_keys:\n",
    "    num_clusters = 5  # Define the number of clusters\n",
    "    spectral = SpectralClustering(n_clusters=num_clusters, affinity='nearest_neighbors')\n",
    "\n",
    "    labels = spectral.fit_predict(feature_matrix)\n",
    "\n",
    "    # Plotting the clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for cluster_index in range(num_clusters):\n",
    "        plt.scatter(np.arange(len(labels))[labels == cluster_index], \n",
    "                    distance_matrices[vid_key][labels == cluster_index, 0],\n",
    "                    label=f'Cluster {cluster_index+1}')\n",
    "\n",
    "    plt.title(f'Spectral Clustering for Video {vid_key}')\n",
    "    plt.xlabel('Frame Index')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d4c38",
   "metadata": {},
   "source": [
    "# Covariance Matrix #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f18476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Center the data by subtracting the column means\n",
    "# centered_distance_matrix = distance_matrix - np.mean(distance_matrix, axis=0)\n",
    "\n",
    "# # Compute the covariance matrix\n",
    "# covariance_matrix = np.dot(centered_distance_matrix.T, centered_distance_matrix) / (num_frames - 1)\n",
    "\n",
    "# # 'covariance_matrix' now contains the covariance matrix of the distance matrix\n",
    "\n",
    "################################################\n",
    "\n",
    "covariance_matrices = {}  # Dictionary to store covariance matrices for each video\n",
    "\n",
    "for vid_key, distance_matrix in distance_matrices.items():\n",
    "    # Center the distance matrix by subtracting the column means\n",
    "    centered_distance = distance_matrix - np.mean(distance_matrix, axis=0)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.dot(centered_distance.T, centered_distance) / (num_frames - 1)\n",
    "\n",
    "    # Store the covariance matrix for this video\n",
    "    covariance_matrices[vid_key] = covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da93fe-a4d6-40e9-9cbb-ab6ed0c7a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of each covariance matrix\n",
    "for vid_key, cov_matrix in covariance_matrices.items():\n",
    "    print(f\"Covariance matrix shape for video {vid_key}: {cov_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1c9cc",
   "metadata": {},
   "source": [
    "GOT HERE! \n",
    "I have already estimated the eigenvectors and eigenvalues and the vairance explained by each eigenvector. The results indicate vary low variability accounted for by the eigenvalues till the last one. I was not expecting this at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944accc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming covariance_matrices is a list of covariance matrices from n videos\n",
    "\n",
    "for vid_key, cov_matrix in covariance_matrices.items():\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cov_matrix, cmap='viridis', origin='upper')\n",
    "    plt.colorbar(label='Covariance')\n",
    "    plt.title(f'Covariance Matrix Heatmap for Video {vid_key}')\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('Image Index')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20cb958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Iterate through each video\n",
    "for vid_key, cov_matrix in covariance_matrices.items():\n",
    "    # Calculate the eigendecomposition of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # Sort eigenvalues in descending order and get sorted indices\n",
    "    ind = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[ind]\n",
    "    eigenvectors = eigenvectors[:, ind]\n",
    "    \n",
    "    # Plotting the eigenvalues for each video\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(eigenvalues, '.-', markersize=5)\n",
    "    plt.xlabel('$k$', fontsize=16) \n",
    "    plt.ylabel('$\\lambda$', fontsize=16)\n",
    "    plt.title(f'Eigenvalues for Video {vid_key}', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and plot the cumulative variance explained for each video\n",
    "    sigma = np.sum(eigenvalues)\n",
    "    sigma_k = np.cumsum(eigenvalues) / sigma\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sigma_k, '.-', markersize=5)\n",
    "    plt.xlabel('$k$', fontsize=16) \n",
    "    plt.ylabel('$Cumulative Variance Explained$', fontsize=16)\n",
    "    plt.title(f'Cumulative Variance Explained for Video {vid_key}', fontsize=16)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 568.844,
   "position": {
    "height": "590.844px",
    "left": "2794px",
    "right": "20px",
    "top": "548px",
    "width": "345px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
